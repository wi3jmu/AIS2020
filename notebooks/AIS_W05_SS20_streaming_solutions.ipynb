{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*Analytical Information Systems*\n",
    "\n",
    "# Worksheet 5 - Big Data and Streaming\n",
    "\n",
    "Matthias Griebel<br>\n",
    "Lehrstuhl für Wirtschaftsinformatik und Informationsmanagement\n",
    "\n",
    "SS 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MapReduce](https://en.wikipedia.org/wiki/MapReduce) is a programming model and an associated implementation for processing and generating big data sets with a parallel, distributed algorithm on a cluster.\n",
    "\n",
    "Let's have a look at the word count example again\n",
    "\n",
    "<img src=\"http://wi-wiki.de/lib/exe/fetch.php?cache=&w=899&h=417&tok=68959d&media=bigdata:mapreducewordcountoverview1.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Input__\n",
    "\n",
    "1. __Splitting__: Prepare the Map() input\n",
    "\n",
    "1. __Mapping__: Run the user-provided Map() code. Each worker node applies the map function to the local data, and writes the output to a temporary storage.\n",
    "\n",
    "1. __Shuffling__: \"Shuffle\" the Map output to the Reduce processors. \n",
    "\n",
    "1. __Reduce__: Run the user-provided Reduce() code. The Reduce processors process each group of output data, per key, in parallel.\n",
    "\n",
    "1. __Final result__: Produce the final output – the MapReduce system collects and sorts all the Reduce output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce Libraries\n",
    "\n",
    "MapReduce libraries have been written in many programming languages, with different levels of optimization. \n",
    "- A popular open-source implementation that has support for distributed shuffles is part of Apache Hadoop.\n",
    "- [RHadoop](https://github.com/RevolutionAnalytics/RHadoop/wiki) is a collection of five R packages that allow users to manage and analyze data with Apache Hadoop. \n",
    "    - using RHadoop requires a Java and Hadoop installation, the Hadoop Distributed File System, etc.\n",
    "\n",
    "Thus, we will only examplify the MapReduce algorithm using basic R and the `tidyverse`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examplary R MapReduce Word Count Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Defining the map function__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map function breaks the line into words and outputs a key/value pair for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words <- function(line){\n",
    "    line %>%\n",
    "            str_split(\" \",simplify=FALSE) %>%\n",
    "            unlist() %>%\n",
    "            tibble(key=., value=1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Defining the reduce function__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the word count example, the Reduce function sums the word counts and generates a single output of the word and the final sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_count <- function(df){\n",
    "    df %>%\n",
    "        summarise(key=key[1],\n",
    "                  count=sum(value))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Going through the MapReduce steps__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Input__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Deer Bear River\\nCar Car River\\nDeer Car Bear'"
      ],
      "text/latex": [
       "'Deer Bear River\\textbackslash{}nCar Car River\\textbackslash{}nDeer Car Bear'"
      ],
      "text/markdown": [
       "'Deer Bear River\\nCar Car River\\nDeer Car Bear'"
      ],
      "text/plain": [
       "[1] \"Deer Bear River\\nCar Car River\\nDeer Car Bear\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Input =  \"Deer Bear River\\nCar Car River\\nDeer Car Bear\"\n",
    "Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Splitting__\n",
    "\n",
    "We will split the input by line ('\\n' indicates a new line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Deer Bear River'</li>\n",
       "\t<li>'Car Car River'</li>\n",
       "\t<li>'Deer Car Bear'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Deer Bear River'\n",
       "\\item 'Car Car River'\n",
       "\\item 'Deer Car Bear'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Deer Bear River'\n",
       "2. 'Car Car River'\n",
       "3. 'Deer Car Bear'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"Deer Bear River\" \"Car Car River\"   \"Deer Car Bear\"  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Input %>%\n",
    "    str_split(\"\\n\",simplify=FALSE) %>% unlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><table>\n",
       "<thead><tr><th scope=col>key</th><th scope=col>value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Deer </td><td>1    </td></tr>\n",
       "\t<tr><td>Bear </td><td>1    </td></tr>\n",
       "\t<tr><td>River</td><td>1    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<thead><tr><th scope=col>key</th><th scope=col>value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Car  </td><td>1    </td></tr>\n",
       "\t<tr><td>Car  </td><td>1    </td></tr>\n",
       "\t<tr><td>River</td><td>1    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<thead><tr><th scope=col>key</th><th scope=col>value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Deer</td><td>1   </td></tr>\n",
       "\t<tr><td>Car </td><td>1   </td></tr>\n",
       "\t<tr><td>Bear</td><td>1   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item \\begin{tabular}{r|ll}\n",
       " key & value\\\\\n",
       "\\hline\n",
       "\t Deer  & 1    \\\\\n",
       "\t Bear  & 1    \\\\\n",
       "\t River & 1    \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\begin{tabular}{r|ll}\n",
       " key & value\\\\\n",
       "\\hline\n",
       "\t Car   & 1    \\\\\n",
       "\t Car   & 1    \\\\\n",
       "\t River & 1    \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\begin{tabular}{r|ll}\n",
       " key & value\\\\\n",
       "\\hline\n",
       "\t Deer & 1   \\\\\n",
       "\t Car  & 1   \\\\\n",
       "\t Bear & 1   \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. \n",
       "| key | value |\n",
       "|---|---|\n",
       "| Deer  | 1     |\n",
       "| Bear  | 1     |\n",
       "| River | 1     |\n",
       "\n",
       "\n",
       "2. \n",
       "| key | value |\n",
       "|---|---|\n",
       "| Car   | 1     |\n",
       "| Car   | 1     |\n",
       "| River | 1     |\n",
       "\n",
       "\n",
       "3. \n",
       "| key | value |\n",
       "|---|---|\n",
       "| Deer | 1    |\n",
       "| Car  | 1    |\n",
       "| Bear | 1    |\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "# A tibble: 3 x 2\n",
       "  key   value\n",
       "  <chr> <dbl>\n",
       "1 Deer      1\n",
       "2 Bear      1\n",
       "3 River     1\n",
       "\n",
       "[[2]]\n",
       "# A tibble: 3 x 2\n",
       "  key   value\n",
       "  <chr> <dbl>\n",
       "1 Car       1\n",
       "2 Car       1\n",
       "3 River     1\n",
       "\n",
       "[[3]]\n",
       "# A tibble: 3 x 2\n",
       "  key   value\n",
       "  <chr> <dbl>\n",
       "1 Deer      1\n",
       "2 Car       1\n",
       "3 Bear      1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Input %>%\n",
    "    str_split(\"\\n\",simplify=FALSE) %>% unlist %>%\n",
    "    map(count_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><table>\n",
       "<thead><tr><th scope=col>key</th><th scope=col>value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Bear</td><td>1   </td></tr>\n",
       "\t<tr><td>Bear</td><td>1   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<thead><tr><th scope=col>key</th><th scope=col>value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Car</td><td>1  </td></tr>\n",
       "\t<tr><td>Car</td><td>1  </td></tr>\n",
       "\t<tr><td>Car</td><td>1  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<thead><tr><th scope=col>key</th><th scope=col>value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Deer</td><td>1   </td></tr>\n",
       "\t<tr><td>Deer</td><td>1   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<thead><tr><th scope=col>key</th><th scope=col>value</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>River</td><td>1    </td></tr>\n",
       "\t<tr><td>River</td><td>1    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item \\begin{tabular}{r|ll}\n",
       " key & value\\\\\n",
       "\\hline\n",
       "\t Bear & 1   \\\\\n",
       "\t Bear & 1   \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\begin{tabular}{r|ll}\n",
       " key & value\\\\\n",
       "\\hline\n",
       "\t Car & 1  \\\\\n",
       "\t Car & 1  \\\\\n",
       "\t Car & 1  \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\begin{tabular}{r|ll}\n",
       " key & value\\\\\n",
       "\\hline\n",
       "\t Deer & 1   \\\\\n",
       "\t Deer & 1   \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\begin{tabular}{r|ll}\n",
       " key & value\\\\\n",
       "\\hline\n",
       "\t River & 1    \\\\\n",
       "\t River & 1    \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. \n",
       "| key | value |\n",
       "|---|---|\n",
       "| Bear | 1    |\n",
       "| Bear | 1    |\n",
       "\n",
       "\n",
       "2. \n",
       "| key | value |\n",
       "|---|---|\n",
       "| Car | 1   |\n",
       "| Car | 1   |\n",
       "| Car | 1   |\n",
       "\n",
       "\n",
       "3. \n",
       "| key | value |\n",
       "|---|---|\n",
       "| Deer | 1    |\n",
       "| Deer | 1    |\n",
       "\n",
       "\n",
       "4. \n",
       "| key | value |\n",
       "|---|---|\n",
       "| River | 1     |\n",
       "| River | 1     |\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "# A tibble: 2 x 2\n",
       "  key   value\n",
       "  <chr> <dbl>\n",
       "1 Bear      1\n",
       "2 Bear      1\n",
       "\n",
       "[[2]]\n",
       "# A tibble: 3 x 2\n",
       "  key   value\n",
       "  <chr> <dbl>\n",
       "1 Car       1\n",
       "2 Car       1\n",
       "3 Car       1\n",
       "\n",
       "[[3]]\n",
       "# A tibble: 2 x 2\n",
       "  key   value\n",
       "  <chr> <dbl>\n",
       "1 Deer      1\n",
       "2 Deer      1\n",
       "\n",
       "[[4]]\n",
       "# A tibble: 2 x 2\n",
       "  key   value\n",
       "  <chr> <dbl>\n",
       "1 River     1\n",
       "2 River     1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Input %>%\n",
    "    str_split(\"\\n\",simplify=FALSE) %>% unlist %>%\n",
    "    map(count_words) %>% \n",
    "    map_df(rbind) %>% group_split(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Reducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><table>\n",
       "<thead><tr><th scope=col>key</th><th scope=col>count</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Bear</td><td>2   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<thead><tr><th scope=col>key</th><th scope=col>count</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Car</td><td>3  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<thead><tr><th scope=col>key</th><th scope=col>count</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Deer</td><td>2   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<thead><tr><th scope=col>key</th><th scope=col>count</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>River</td><td>2    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item \\begin{tabular}{r|ll}\n",
       " key & count\\\\\n",
       "\\hline\n",
       "\t Bear & 2   \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\begin{tabular}{r|ll}\n",
       " key & count\\\\\n",
       "\\hline\n",
       "\t Car & 3  \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\begin{tabular}{r|ll}\n",
       " key & count\\\\\n",
       "\\hline\n",
       "\t Deer & 2   \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \\begin{tabular}{r|ll}\n",
       " key & count\\\\\n",
       "\\hline\n",
       "\t River & 2    \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. \n",
       "| key | count |\n",
       "|---|---|\n",
       "| Bear | 2    |\n",
       "\n",
       "\n",
       "2. \n",
       "| key | count |\n",
       "|---|---|\n",
       "| Car | 3   |\n",
       "\n",
       "\n",
       "3. \n",
       "| key | count |\n",
       "|---|---|\n",
       "| Deer | 2    |\n",
       "\n",
       "\n",
       "4. \n",
       "| key | count |\n",
       "|---|---|\n",
       "| River | 2     |\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "# A tibble: 1 x 2\n",
       "  key   count\n",
       "  <chr> <dbl>\n",
       "1 Bear      2\n",
       "\n",
       "[[2]]\n",
       "# A tibble: 1 x 2\n",
       "  key   count\n",
       "  <chr> <dbl>\n",
       "1 Car       3\n",
       "\n",
       "[[3]]\n",
       "# A tibble: 1 x 2\n",
       "  key   count\n",
       "  <chr> <dbl>\n",
       "1 Deer      2\n",
       "\n",
       "[[4]]\n",
       "# A tibble: 1 x 2\n",
       "  key   count\n",
       "  <chr> <dbl>\n",
       "1 River     2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Input %>%\n",
    "    str_split(\"\\n\",simplify=FALSE) %>% unlist %>%\n",
    "    map(count_words) %>% \n",
    "    map_df(rbind) %>% group_split(key) %>%\n",
    "    map(reduce_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Merge and sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>key</th><th scope=col>count</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Car  </td><td>3    </td></tr>\n",
       "\t<tr><td>Bear </td><td>2    </td></tr>\n",
       "\t<tr><td>Deer </td><td>2    </td></tr>\n",
       "\t<tr><td>River</td><td>2    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " key & count\\\\\n",
       "\\hline\n",
       "\t Car   & 3    \\\\\n",
       "\t Bear  & 2    \\\\\n",
       "\t Deer  & 2    \\\\\n",
       "\t River & 2    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| key | count |\n",
       "|---|---|\n",
       "| Car   | 3     |\n",
       "| Bear  | 2     |\n",
       "| Deer  | 2     |\n",
       "| River | 2     |\n",
       "\n"
      ],
      "text/plain": [
       "  key   count\n",
       "1 Car   3    \n",
       "2 Bear  2    \n",
       "3 Deer  2    \n",
       "4 River 2    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Input %>%\n",
    "    str_split(\"\\n\",simplify=FALSE) %>% unlist %>%\n",
    "    map(count_words) %>% \n",
    "    map_df(rbind) %>% group_split(key) %>%\n",
    "    map(reduce_count) %>%\n",
    "    map_df(cbind) %>% arrange(desc(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Doing it the undistributed tidyverse way__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>key</th><th scope=col>count</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Car  </td><td>3    </td></tr>\n",
       "\t<tr><td>Bear </td><td>2    </td></tr>\n",
       "\t<tr><td>Deer </td><td>2    </td></tr>\n",
       "\t<tr><td>River</td><td>2    </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " key & count\\\\\n",
       "\\hline\n",
       "\t Car   & 3    \\\\\n",
       "\t Bear  & 2    \\\\\n",
       "\t Deer  & 2    \\\\\n",
       "\t River & 2    \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| key | count |\n",
       "|---|---|\n",
       "| Car   | 3     |\n",
       "| Bear  | 2     |\n",
       "| Deer  | 2     |\n",
       "| River | 2     |\n",
       "\n"
      ],
      "text/plain": [
       "  key   count\n",
       "1 Car   3    \n",
       "2 Bear  2    \n",
       "3 Deer  2    \n",
       "4 River 2    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Input %>%\n",
    "    str_replace_all(\"\\n\", \" \") %>%\n",
    "    str_split(\" \",simplify=FALSE) %>% unlist %>% \n",
    "    tibble(key=.) %>%\n",
    "    group_by(key) %>%\n",
    "    summarize(count=n()) %>%  arrange(desc(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Credits__\n",
    "\n",
    "- Jure Leskovec, Stanford University, http://web.stanford.edu/class/cs246/slides/15-streams1.pdf\n",
    "- Michael Freedman, Princeton University, https://www.cs.princeton.edu/courses/archive/fall16/cos418/docs/L22-stream-processing.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In many data mining situations, we do not know the entire data set in advance\n",
    "- We can think of the data as infinite and non-stationary (the distribution changes over time)\n",
    "- Stream Management is important when the input rate is controlled externally:\n",
    "    - Google queries\n",
    "    - Twitter or Facebook status updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The Stream Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input elements enter at a rapid rate, at one or more input ports (i.e., streams)\n",
    "    - We call elements of the stream tuples\n",
    "- The system cannot store the entire stream accessibly\n",
    "\n",
    "    \n",
    "How do you make critical calculations about the stream using a limited amount of (secondary) memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Stream Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stateless conversion__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/wi3jmu/AIS2020/master/notebooks/figures/05/sc_ctoF.pdf\" style=\"width:20%\">\n",
    "\n",
    "- Convert Celsius temperature to Fahrenheit: __emit__ (input * 9 / 5) + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stateless filtering__\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wi3jmu/AIS2020/master/notebooks/figures/05/sc_sf.pdf\" style=\"width:20%\">\n",
    "\n",
    "Function can filter inputs: –if(input>threshold) {__emit__ input}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stateful conversion__\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wi3jmu/AIS2020/master/notebooks/figures/05/sc_ewa.pdf\" style=\"width:20%\">\n",
    "\n",
    "Compute EWMA of Fahrenheit temperature:\n",
    "- new_temp = ⍺ * ( CtoF(input) ) + (1- ⍺) * last_temp\n",
    "- last_temp = new_temp – emit new_temp\n",
    "- emit new_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Aggregation (stateful)__\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wi3jmu/AIS2020/master/notebooks/figures/05/sc_agg.pdf\" style=\"width:20%\">\n",
    "\n",
    "E.g.,Average value per window\n",
    "- Window can be # elements (10) or time (1s)\n",
    "- Windows can be disjoint (every 5s)\n",
    "- Windows can be “tumbling” (5s window every 1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stream processing as chain__\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wi3jmu/AIS2020/master/notebooks/figures/05/sc_chain.pdf\" style=\"width:20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stream processing as directed graph__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/wi3jmu/AIS2020/master/notebooks/figures/05/sc_chain.pdf\" style=\"width:20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The challenge of stream processing for BIG DATA\n",
    "\n",
    "Large amounts of data to process in realtime\n",
    "\n",
    "__Examples__:\n",
    "- Social network trends (#trending)\n",
    "- Intrusion detection systems (networks, datacenters)\n",
    "- Sensors: Detect earthquakes by correlating vibrations of millions of smartphones\n",
    "- Fraud detection\n",
    "    - Visa: 2000 txn / sec on average, peak ~47,000 / sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stateless operations: trivially parallelized__\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wi3jmu/AIS2020/master/notebooks/figures/05/scale_out.pdf\" style=\"width:20%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__State complicates parallelization__\n",
    "\n",
    "\n",
    "- Need to join results across parallel computations\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wi3jmu/AIS2020/master/notebooks/figures/05/agg_par.pdf\" style=\"width:30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parallelization complicates fault-tolerance__\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wi3jmu/AIS2020/master/notebooks/figures/05/fault.pdf\" style=\"width:30%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We can parallelize joins__\n",
    "\n",
    "- using partitioned hash joins\n",
    "- but agian, complicates fault-tolerance\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/wi3jmu/AIS2020/master/notebooks/figures/05/par_joins.pdf\" style=\"width:30%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream Processing frameworks\n",
    "\n",
    "Different frameworks handle these challenges differently\n",
    "\n",
    "- Record acknowledgement (Storm)\n",
    "- Micro-batches (Spark Streaming, Storm Trident) \n",
    "- Transactional updates (GoogleClouddataflow) \n",
    "- Distributed snapshots (Flink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming data with R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The `sparklyr` interface for Spark Streaming__\n",
    "\n",
    "from the official [Website](https://spark.rstudio.com/guides/streaming/):\n",
    "\n",
    "Spark Streaming makes it easy to build scalable fault-tolerant streaming applications. Because is part of the Spark API, it is possible to re-use query code that queries the current state of the stream, as well as joining the streaming data with historical data. Please see Spark’s official documentation for a deeper look into Spark Streaming.\n",
    "\n",
    "The sparklyr interface provides the following:\n",
    "\n",
    "- Ability to run dplyr, SQL, spark_apply(), and PipelineModels against a stream\n",
    "- Read in multiple formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\n",
    "- Write stream results to Spark memory and the following file formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc\n",
    "- An out-of-the box graph visualization to monitor the stream\n",
    "- A new reactiveSpark() function, that allows Shiny apps to poll the contents of the stream create Shiny apps that are able to read the contents of the stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interacting with a stream\n",
    "\n",
    "A good way of looking at the way how Spark streams update is as a three stage operation:\n",
    "\n",
    "1. __Input__ - Spark reads the data inside a given folder. The folder is expected to contain multiple data files, with new files being created containing the most current stream data.\n",
    "\n",
    "1. __Processing__ - Spark applies the desired operations on top of the data. These operations could be data manipulations (dplyr, SQL), data transformations (sdf operations, PipelineModel predictions), or native R manipulations (spark_apply()).\n",
    "\n",
    "1. __Output__ - The results of processing the input files are saved in a different folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sparklyr` Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Install requirements__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system(\"apt-get install openjdk-8-jdk-headless -qq > /dev/null\")\n",
    "Sys.setenv(JAVA_HOME = \"/usr/lib/jvm/java-8-openjdk-amd64\")\n",
    "install.packages(c(\"sparklyr\", \"future\"))\n",
    "library(sparklyr)\n",
    "spark_install()\n",
    "library(future)\n",
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open the Spark connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in spark_connect(master = \"local\"): could not find function \"spark_connect\"\n",
     "output_type": "error",
     "traceback": [
      "Error in spark_connect(master = \"local\"): could not find function \"spark_connect\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "sc <- spark_connect(master = \"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional step. This resets the input and output folders. It makes it easier to run the code multiple times in a clean manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(file.exists(\"source\")) unlink(\"source\", TRUE)\n",
    "if(file.exists(\"source-out\")) unlink(\"source-out\", TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Produce a single test file inside the “source” folder. This allows the “read” function to infer CSV file definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_generate_test(iterations = 1)\n",
    "list.files(\"source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Point the stream reader to the folder where the streaming files will be placed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_folder <- stream_read_csv(sc, \"source\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Process stream function: The stream_watermark() functions add a new timestamp variable that is then used in the group_by() command. This is required by Spark Stream to accept summarized results as output of the stream. The second step is to simply decide what kinds of aggregations we need to perform. In this case, a simply max, min and count are performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_stream <- read_folder %>%\n",
    "  stream_watermark() %>%\n",
    "  group_by(timestamp) %>%\n",
    "  summarise(\n",
    "    max_x = max(x, na.rm = TRUE),\n",
    "    min_x = min(x, na.rm = TRUE),\n",
    "    count = n()\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The output writer is what starts the streaming job. It will start monitoring the input folder, and then write the new results in the “source-out” folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_output <- stream_write_memory(process_stream, name = \"stream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The test generation function will run 100 files every 0.2 seconds. To run the tests “out-of-sync” with the current R session, the future package is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invisible(future(stream_generate_test(interval = 0.2, iterations = 100))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. The “source-out” folder can be treated as a if it was a single table within Spark. Using spark_read_csv(), the data can be mapped, but not brought into memory (memory = FALSE). This allows the current results to be further analyzed using regular dplyr commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_read_csv(sc, \"stream\", \"source-out\", memory = FALSE) %>%\n",
    "    summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales analysis \n",
    "\n",
    "Calculates the total profit for each product id within eah subset\n",
    "\n",
    "Adds up the profit for each different product id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed with column specification:\n",
      "cols(\n",
      "  date = col_date(format = \"\"),\n",
      "  customerID = col_double(),\n",
      "  productID = col_double(),\n",
      "  payment = col_character(),\n",
      "  amount = col_double(),\n",
      "  price = col_double(),\n",
      "  cost = col_double(),\n",
      "  category = col_character()\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>date</th><th scope=col>customerID</th><th scope=col>productID</th><th scope=col>payment</th><th scope=col>amount</th><th scope=col>price</th><th scope=col>cost</th><th scope=col>category</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>2017-01-16 </td><td>64292      </td><td> 8403      </td><td>paypal     </td><td>3          </td><td>560.74     </td><td>234.89     </td><td>emergency  </td></tr>\n",
       "\t<tr><td>2017-08-16 </td><td>41174      </td><td> 7234      </td><td>paypal     </td><td>3          </td><td>351.14     </td><td>171.11     </td><td>specialty  </td></tr>\n",
       "\t<tr><td>2017-10-26 </td><td>49737      </td><td>32738      </td><td>paypal     </td><td>3          </td><td>343.38     </td><td>105.14     </td><td>emergency  </td></tr>\n",
       "\t<tr><td>2017-11-24 </td><td>24021      </td><td>70159      </td><td>cash       </td><td>2          </td><td>905.96     </td><td>345.40     </td><td>emergency  </td></tr>\n",
       "\t<tr><td>2017-02-13 </td><td>78762      </td><td> 2002      </td><td>cash       </td><td>2          </td><td>799.99     </td><td>407.30     </td><td>emergency  </td></tr>\n",
       "\t<tr><td>2017-07-18 </td><td>79148      </td><td>86205      </td><td>credit card</td><td>1          </td><td>284.07     </td><td>132.35     </td><td>emergency  </td></tr>\n",
       "\t<tr><td>2017-08-23 </td><td>79148      </td><td>40784      </td><td>cash       </td><td>3          </td><td>125.79     </td><td> 47.53     </td><td>specialty  </td></tr>\n",
       "\t<tr><td>2017-11-06 </td><td>23090      </td><td>16224      </td><td>paypal     </td><td>3          </td><td> 85.77     </td><td> 36.61     </td><td>specialty  </td></tr>\n",
       "\t<tr><td>2017-09-28 </td><td>12307      </td><td>82560      </td><td>credit card</td><td>2          </td><td>658.88     </td><td>330.44     </td><td>emergency  </td></tr>\n",
       "\t<tr><td>2017-04-19 </td><td>45757      </td><td>27578      </td><td>credit card</td><td>1          </td><td>458.31     </td><td>269.80     </td><td>emergency  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllll}\n",
       " date & customerID & productID & payment & amount & price & cost & category\\\\\n",
       "\\hline\n",
       "\t 2017-01-16  & 64292       &  8403       & paypal      & 3           & 560.74      & 234.89      & emergency  \\\\\n",
       "\t 2017-08-16  & 41174       &  7234       & paypal      & 3           & 351.14      & 171.11      & specialty  \\\\\n",
       "\t 2017-10-26  & 49737       & 32738       & paypal      & 3           & 343.38      & 105.14      & emergency  \\\\\n",
       "\t 2017-11-24  & 24021       & 70159       & cash        & 2           & 905.96      & 345.40      & emergency  \\\\\n",
       "\t 2017-02-13  & 78762       &  2002       & cash        & 2           & 799.99      & 407.30      & emergency  \\\\\n",
       "\t 2017-07-18  & 79148       & 86205       & credit card & 1           & 284.07      & 132.35      & emergency  \\\\\n",
       "\t 2017-08-23  & 79148       & 40784       & cash        & 3           & 125.79      &  47.53      & specialty  \\\\\n",
       "\t 2017-11-06  & 23090       & 16224       & paypal      & 3           &  85.77      &  36.61      & specialty  \\\\\n",
       "\t 2017-09-28  & 12307       & 82560       & credit card & 2           & 658.88      & 330.44      & emergency  \\\\\n",
       "\t 2017-04-19  & 45757       & 27578       & credit card & 1           & 458.31      & 269.80      & emergency  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| date | customerID | productID | payment | amount | price | cost | category |\n",
       "|---|---|---|---|---|---|---|---|\n",
       "| 2017-01-16  | 64292       |  8403       | paypal      | 3           | 560.74      | 234.89      | emergency   |\n",
       "| 2017-08-16  | 41174       |  7234       | paypal      | 3           | 351.14      | 171.11      | specialty   |\n",
       "| 2017-10-26  | 49737       | 32738       | paypal      | 3           | 343.38      | 105.14      | emergency   |\n",
       "| 2017-11-24  | 24021       | 70159       | cash        | 2           | 905.96      | 345.40      | emergency   |\n",
       "| 2017-02-13  | 78762       |  2002       | cash        | 2           | 799.99      | 407.30      | emergency   |\n",
       "| 2017-07-18  | 79148       | 86205       | credit card | 1           | 284.07      | 132.35      | emergency   |\n",
       "| 2017-08-23  | 79148       | 40784       | cash        | 3           | 125.79      |  47.53      | specialty   |\n",
       "| 2017-11-06  | 23090       | 16224       | paypal      | 3           |  85.77      |  36.61      | specialty   |\n",
       "| 2017-09-28  | 12307       | 82560       | credit card | 2           | 658.88      | 330.44      | emergency   |\n",
       "| 2017-04-19  | 45757       | 27578       | credit card | 1           | 458.31      | 269.80      | emergency   |\n",
       "\n"
      ],
      "text/plain": [
       "   date       customerID productID payment     amount price  cost   category \n",
       "1  2017-01-16 64292       8403     paypal      3      560.74 234.89 emergency\n",
       "2  2017-08-16 41174       7234     paypal      3      351.14 171.11 specialty\n",
       "3  2017-10-26 49737      32738     paypal      3      343.38 105.14 emergency\n",
       "4  2017-11-24 24021      70159     cash        2      905.96 345.40 emergency\n",
       "5  2017-02-13 78762       2002     cash        2      799.99 407.30 emergency\n",
       "6  2017-07-18 79148      86205     credit card 1      284.07 132.35 emergency\n",
       "7  2017-08-23 79148      40784     cash        3      125.79  47.53 specialty\n",
       "8  2017-11-06 23090      16224     paypal      3       85.77  36.61 specialty\n",
       "9  2017-09-28 12307      82560     credit card 2      658.88 330.44 emergency\n",
       "10 2017-04-19 45757      27578     credit card 1      458.31 269.80 emergency"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sales <- read_csv('sales.csv')\n",
    "sales %>% head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_profit <- function(df){\n",
    "    df %>%\n",
    "        # Write your code here \n",
    "        mutate(profit=(price-cost)*amount) %>% \n",
    "        group_by(productID) %>%\n",
    "        summarise(total_profit = sum(profit))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_profit <- function(df){\n",
    "    df %>%\n",
    "        # Write your code here \n",
    "        summarise(productID=productID[1],\n",
    "                  total_profit=sum(total_profit))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>productID</th><th scope=col>total_profit</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>43446    </td><td>171232.25</td></tr>\n",
       "\t<tr><td>24221    </td><td>111657.54</td></tr>\n",
       "\t<tr><td>60974    </td><td> 88118.82</td></tr>\n",
       "\t<tr><td>70159    </td><td> 84644.56</td></tr>\n",
       "\t<tr><td>89266    </td><td> 73408.04</td></tr>\n",
       "\t<tr><td>86064    </td><td> 68414.58</td></tr>\n",
       "\t<tr><td> 9947    </td><td> 64715.40</td></tr>\n",
       "\t<tr><td>61070    </td><td> 64571.40</td></tr>\n",
       "\t<tr><td>62077    </td><td> 63238.50</td></tr>\n",
       "\t<tr><td> 2002    </td><td> 62437.71</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " productID & total\\_profit\\\\\n",
       "\\hline\n",
       "\t 43446     & 171232.25\\\\\n",
       "\t 24221     & 111657.54\\\\\n",
       "\t 60974     &  88118.82\\\\\n",
       "\t 70159     &  84644.56\\\\\n",
       "\t 89266     &  73408.04\\\\\n",
       "\t 86064     &  68414.58\\\\\n",
       "\t  9947     &  64715.40\\\\\n",
       "\t 61070     &  64571.40\\\\\n",
       "\t 62077     &  63238.50\\\\\n",
       "\t  2002     &  62437.71\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| productID | total_profit |\n",
       "|---|---|\n",
       "| 43446     | 171232.25 |\n",
       "| 24221     | 111657.54 |\n",
       "| 60974     |  88118.82 |\n",
       "| 70159     |  84644.56 |\n",
       "| 89266     |  73408.04 |\n",
       "| 86064     |  68414.58 |\n",
       "|  9947     |  64715.40 |\n",
       "| 61070     |  64571.40 |\n",
       "| 62077     |  63238.50 |\n",
       "|  2002     |  62437.71 |\n",
       "\n"
      ],
      "text/plain": [
       "   productID total_profit\n",
       "1  43446     171232.25   \n",
       "2  24221     111657.54   \n",
       "3  60974      88118.82   \n",
       "4  70159      84644.56   \n",
       "5  89266      73408.04   \n",
       "6  86064      68414.58   \n",
       "7   9947      64715.40   \n",
       "8  61070      64571.40   \n",
       "9  62077      63238.50   \n",
       "10  2002      62437.71   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sales %>% #Input\n",
    "    split(sample(rep(1:5, 1000))) %>% #Splitting\n",
    "    map(calculate_profit) %>% #Mapping\n",
    "    map_df(rbind) %>% group_split(productID)%>% #Shuffling\n",
    "    map(reduce_profit) %>% #Reduce\n",
    "    map_df(cbind) %>% arrange(desc(total_profit)) %>%  #Merge and Sort\n",
    "    head(10) #Display only top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why do stateful operation complicate parallelization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Need to join results across parallel computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Why do parallelization operations complicate fault-tolerance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How to ensure exactly-once semantics if one node fails?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  },
  "rise": {
   "enable_chalkboard": false,
   "overlay": "<div class='logo'><img src='images/uniwue4c.png'></div>",
   "scroll": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Agenda",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
